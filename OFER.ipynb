{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " OFER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sneakatyou/OFER/blob/main/OFER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSAxHAZ9ZfSq"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY3_o2K2AFxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b85d1b4-2fc6-40e6-b081-dcef3420c90e"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq4UjLaoB853"
      },
      "source": [
        "WORKERS = tf.data.experimental.AUTOTUNE\n",
        "VAL_SPLIT = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tsVQo5jDZIy"
      },
      "source": [
        "class Dataset():\n",
        "  def __init__(self, path, batch_size = 32, image_shape = (88, 88, 3), create_occlusion = False):\n",
        "    self.path = path\n",
        "    self.batch_size = batch_size\n",
        "    self.image_height = image_shape[0]\n",
        "    self.image_width = image_shape[1]\n",
        "    self.label_dict = {'surprise': 0, 'fear': 1, 'disgust': 2, 'happiness': 3, 'sadness': 4, 'anger': 5, 'neutral': 6}\n",
        "    self.num_classes = len(list(self.label_dict))\n",
        "    self.all_paths, self.all_labels = [], []\n",
        "    self.X_train, self.X_test, self.Y_train, self.Y_test = [], [], [], []\n",
        "    for subdir, dirs, files in os.walk(self.path):\n",
        "      for f in files:\n",
        "        image_path = os.path.join(subdir, f)\n",
        "        if f[:5] == 'train':\n",
        "          self.X_train.append(image_path)\n",
        "          label = image_path.split('/')[-2]\n",
        "          self.Y_train.append(self.label_dict[label])\n",
        "        else:\n",
        "          self.X_test.append(image_path)\n",
        "          label = image_path.split('/')[-2]\n",
        "          self.Y_test.append(self.label_dict[label])\n",
        "\n",
        "\n",
        "  def parse_function(self, image_path, label):\n",
        "   \n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels = 3)\n",
        "    image = tf.image.resize(image, [self.image_width, self.image_height])\n",
        "    label = tf.one_hot(label,self.num_classes)\n",
        "    return image,label\n",
        "  \n",
        "  def get_train_ds(self):\n",
        "    BUFFER_SIZE = len(self.X_train)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((self.X_train , self.Y_train))\n",
        "    ds = ds.shuffle(BUFFER_SIZE)\n",
        "    ds = ds.repeat()\n",
        "    ds = ds.map(self.parse_function, num_parallel_calls = WORKERS)\n",
        "    ds = ds.batch(self.batch_size, drop_remainder = True)\n",
        "    ds = ds.prefetch(1)\n",
        "    return ds\n",
        "  \n",
        "  def get_test_ds(self):\n",
        "    BUFFER_SIZE = len(self.X_test)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((self.X_test , self.Y_test))\n",
        "    ds = ds.shuffle(BUFFER_SIZE)\n",
        "    ds = ds.repeat(count=1)\n",
        "    ds = ds.map(self.parse_function, num_parallel_calls = WORKERS)\n",
        "    ds = ds.batch( self.batch_size, drop_remainder = True )\n",
        "    ds = ds.prefetch(1)\n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc-zlA6g5-ph"
      },
      "source": [
        "class OcclusionCreator():\n",
        "  def __init__(self):\n",
        "    #path to folder which contains images to be used as occlusions\n",
        "    path = '/content/drive/My Drive/Occluded Facial Expression Recognition/Datasets/occlusion/www.pngplay.com'\n",
        "    self.occlusion_paths = []\n",
        "    for subdir, dirs, files in os.walk(path):\n",
        "      for f in files:\n",
        "        image_path = os.path.join(subdir, f)\n",
        "        self.occlusion_paths.append(image_path)\n",
        "    self.index = 0\n",
        "  \n",
        "  def resize(self, image_1, image_2):\n",
        "    x1 = image_1.shape[1]\n",
        "    y1 = image_1.shape[0]\n",
        "    x2 = image_2.shape[1]\n",
        "    y2 = image_2.shape[0]\n",
        "    r = x2 // y2\n",
        "\n",
        "    if r == 0:\n",
        "      r = y2 // x2\n",
        "      f = np.random.rand(30, 55)\n",
        "      factor = np.random.randint(10, 15)\n",
        "      factor = factor / 10\n",
        "      seedY = np.random.randint(30, 55)\n",
        "      seedX = int((seedY / r)*factor)\n",
        "    elif r in (0.8, 1.2):\n",
        "      seedX = np.random.randint(30, 55)\n",
        "      seedY = int((seedX / r)*factor)     \n",
        "    else:\n",
        "      f = np.random.rand(30, 55)\n",
        "      factor = np.random.randint(10, 15)\n",
        "      factor = factor / 10\n",
        "      seedX = np.random.randint(30, 55)\n",
        "      seedY = int((seedX / r)*factor)\n",
        "\n",
        "    if(x2 > x1 or y2 > y1 or x2 > x1 // 2 or y2 > x2 // 2):\n",
        "      image_2 = cv2.resize(image_2, (seedX, seedY), interpolation = cv2.INTER_AREA) \n",
        "    return image_2\n",
        "\n",
        "  def generate_coordinates(self, image_1, image_2):\n",
        "    x1 = image_1.shape[1]\n",
        "    y1 = image_1.shape[0]\n",
        "    x2 = image_2.shape[1]\n",
        "    y2 = image_2.shape[0]\n",
        "    try:\n",
        "      seedX = np.random.randint(0, (x1 - x2))\n",
        "      seedY = np.random.randint(0, (y1 - y2))\n",
        "    except:\n",
        "      seedX = 50\n",
        "      seedY = 50\n",
        "    return seedX, seedY\n",
        "\n",
        "  def overlay(self, image, occlusion, coordinates):\n",
        "    x_offset, y_offset = coordinates[0], coordinates[1]\n",
        "    x1, x2 = x_offset, x_offset + occlusion.shape[1]\n",
        "    y1, y2 = y_offset, y_offset + occlusion.shape[0]\n",
        "    occ_alpha = (occlusion[:, :, 3] / 255.0)\n",
        "    img_alpha = 1.0 - occ_alpha\n",
        "    for c in range(0, 3):\n",
        "      image[y1:y2, x1:x2, c] = (occ_alpha*occlusion[:, :, c] + img_alpha*image[y1:y2, x1:x2, c])\n",
        "    return image\n",
        "\n",
        "  def impose(self, x_batch):\n",
        "    occluded_images = []\n",
        "    for i in range(x_batch.shape[0]):\n",
        "      try:\n",
        "        image = x_batch[i]\n",
        "        occlusion_path = self.occlusion_paths[self.index]\n",
        "        self.index = (self.index + 1) % len(self.occlusion_paths)\n",
        "        occlusion = cv2.imread(occlusion_path, cv2.IMREAD_UNCHANGED)\n",
        "        occlusion = self.resize(image, occlusion)\n",
        "        coordinates = self.generate_coordinates(image, occlusion)\n",
        "        occluded_image = self.overlay(image, occlusion, coordinates)\n",
        "        occluded_images.append(occluded_image)\n",
        "      except:\n",
        "        print(self.occlusion_paths[self.index - 1])\n",
        "    return tf.convert_to_tensor(occluded_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB8nQhrqbWNW"
      },
      "source": [
        "MODEL DESCRIPTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyVxWK6ZdHFI"
      },
      "source": [
        "class BaseModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(BaseModel, self).__init__()\n",
        "    self.dense = tf.keras.layers.Dense(units = 7)\n",
        "    self.input_layer = tf.keras.Input(shape = (88, 88, 3))\n",
        "    self.dropout = tf.keras.layers.Dropout(rate=0.6,seed=7)\n",
        "    self.batchNorm=tf.keras.layers.BatchNormalization()\n",
        "    self.GlobalAvg=tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self.base = tf.keras.applications.resnet50.ResNet50(weights = 'imagenet', input_tensor = self.input_layer, include_top = False)\n",
        "    for layer in self.base.layers:\n",
        "      if layer.__class__.__name__ == 'BatchNormalization':\n",
        "        layer.trainable = False\n",
        "    output = self.base.get_layer('conv3_block4_3_conv').output\n",
        "    self.model = tf.keras.Model(inputs = [self.base.input], outputs = [output])\n",
        "\n",
        "  def call(self, x):\n",
        "    resnet = self.base\n",
        "    \n",
        "    x=tf.keras.applications.resnet50.preprocess_input(x)\n",
        "    z = resnet(x)\n",
        "    z=self.dropout(z)\n",
        "    z=self.GlobalAvg(z)\n",
        "    z=self.dense(z)\n",
        "    z=self.batchNorm(z)   \n",
        "    z = tf.keras.activations.softmax(z)\n",
        "    return z\n",
        "\n",
        "  def feature_map(self, x):\n",
        "    return self.model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nS9JDVeBmg"
      },
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (2, 2), padding = 'same')\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (2, 2), padding = 'same')\n",
        "    self.conv3 = tf.keras.layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (2, 2), padding = 'same')\n",
        "    self.conv4 = tf.keras.layers.Conv2D(filters = 512, kernel_size = (3, 3), strides = (2, 2), padding = 'same')\n",
        "    self.conv5 = tf.keras.layers.Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), padding = 'same') #-- padding changed to same\n",
        "\n",
        "  def call(self, h):\n",
        "    z = self.conv1(h)\n",
        "    z = tf.nn.leaky_relu(z)\n",
        "\n",
        "    z = self.conv2(z)\n",
        "    z = tf.keras.layers.BatchNormalization()(z)\n",
        "    z = tf.nn.leaky_relu(z)\n",
        "\n",
        "    z = self.conv3(z)\n",
        "    z = tf.keras.layers.BatchNormalization()(z)\n",
        "    z = tf.nn.leaky_relu(z)\n",
        "\n",
        "    z = self.conv4(z)\n",
        "    z = tf.keras.layers.BatchNormalization()(z)\n",
        "    z = tf.nn.leaky_relu(z)\n",
        "\n",
        "    z = self.conv5(z)\n",
        "\n",
        "    logits = tf.squeeze(z)\n",
        "    z = tf.nn.sigmoid(z) + 1e-10\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu2s4Glo2o0v"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.deconv1 = tf.keras.layers.Conv2DTranspose(filters = 256, kernel_size = (3, 3), strides = (2, 2), padding = 'same')\n",
        "    self.deconv2 = tf.keras.layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides = (2, 2), padding = 'same')\n",
        "    self.deconv3 = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = (3, 3), strides = (2, 2), padding = 'same')\n",
        "    \n",
        "  def call(self, h):\n",
        "    z = self.deconv1(h)\n",
        "    z = tf.keras.layers.BatchNormalization()(z)\n",
        "    z = tf.keras.activations.relu(z)\n",
        "\n",
        "    z = self.deconv2(z)\n",
        "    z = tf.keras.layers.BatchNormalization()(z)\n",
        "    z = tf.keras.activations.relu(z)\n",
        "\n",
        "    z = self.deconv3(z)\n",
        "    z = tf.keras.layers.BatchNormalization()(z)\n",
        "    z = tf.keras.activations.relu(z)\n",
        "\n",
        "    reconstructed_image = tf.math.scalar_mul(255, z)\n",
        "    return reconstructed_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln6tSXK_Gf9p"
      },
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.occluded_net = BaseModel()\n",
        "    self.non_occluded_net = BaseModel()\n",
        "    self.discriminator = Discriminator()\n",
        "    self.decoder = Decoder()\n",
        "    self.occ_model = None\n",
        "    self.occlusion_creator_1 = OcclusionCreator()\n",
        "    self.occlusion_creator_2 = OcclusionCreator()\n",
        "\n",
        "  def train_occluded_net(self, train_ds, test_ds, num_epochs = 100, learning_rate = 0.0001, batch_size = 32):\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate = learning_rate)\n",
        "    \n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(min_delta = 0.00001, mode = 'min', patience = 10)\n",
        "    ckpt_path = '/content/drive/My Drive/Occluded Facial Expression Recognition/Occluded Checkpoints'\n",
        "    #checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(ckpt_path, 'Ckpt_{epoch:02d}_{val_loss:.2f}.ckpt'), save_best_only = True, save_weights_only = True)\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learning_rate*tf.math.exp(-0.01*epoch))\n",
        "\n",
        "    self.occluded_net.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "    self.occluded_net.fit(train_ds, epochs = num_epochs, steps_per_epoch = 200, \n",
        "                          validation_data = test_ds, \n",
        "                          callbacks = [early_stopping, lr_scheduler]) #, checkpoint\n",
        "    \n",
        "  def train_non_occluded_net(self, train_ds, test_ds, num_epochs = 100, learning_rate = 0.0001, batch_size = 32):\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate = learning_rate)\n",
        "    \n",
        "    early_stopping = tf.python.keras.callbacks.EarlyStopping(min_delta = 0.00001, mode = 'min', patience = 10)\n",
        "    ckpt_path = '/content/drive/My Drive/Occluded Facial Expression Recognition/Non-occluded Checkpoints'\n",
        "    #checkpoint = tf.python.keras.callbacks.ModelCheckpoint(os.path.join(ckpt_path, 'Ckpt_{epoch:02d}_{val_loss:.2f}.ckpt'), save_best_only = True, save_weights_only = True)\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learning_rate*tf.math.exp(-0.01*epoch))\n",
        "\n",
        "    self.non_occluded_net.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "    self.non_occluded_net.fit(train_ds, epochs = num_epochs, steps_per_epoch = 200, \n",
        "                              validation_data = test_ds,\n",
        "                              callbacks = [early_stopping,lr_scheduler]) #, checkpoint\n",
        "\n",
        "  def build_architecture(self):\n",
        "    x = tf.keras.Input(shape = (88, 88, 3))\n",
        "    h = tf.keras.Input(shape = (11, 11, 512))\n",
        "    y = self.occluded_net(x)\n",
        "    x_rec = self.decoder(h)\n",
        "    model = tf.keras.Model(inputs = [x, h], outputs = [y, x_rec])\n",
        "    return model\n",
        "  \n",
        "  def train(self, train_ds, test_ds, lambdas, num_epochs = 100, k1 = 50, k2 = 50, disc_lr = 0.0001, occ_lr = 0.00002, batch_size = 32):\n",
        "    self.occ_model = self.build_architecture()\n",
        "    disc_optimizer = tf.keras.optimizers.Adam(learning_rate = disc_lr)\n",
        "    occ_optimizer = tf.keras.optimizers.Adam(learning_rate = occ_lr)\n",
        "    disc_loss = tf.keras.metrics.Mean(name = 'disc_loss')\n",
        "    occ_loss = tf.keras.metrics.Mean(name = 'occ_loss')\n",
        "    occ_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'occ_accuracy')\n",
        "    disc_loss_log = []\n",
        "    occ_loss_log = []\n",
        "    accuracy_log = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      disc_loss.reset_states()\n",
        "      batch_iter = iter(train_ds)\n",
        "      for k in range(k1):\n",
        "        x_batch, _ = next(batch_iter)\n",
        "        occ_x_batch = self.occlusion_creator_1.impose(x_batch.numpy())\n",
        "        ho_batch = self.occluded_net.feature_map(occ_x_batch)\n",
        "        hc_batch = self.non_occluded_net.feature_map(x_batch)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "          d_batch = tf.ones((batch_size, ))\n",
        "          d_batch2 = tf.zeros((batch_size,))\n",
        "          d_pred_fake = self.discriminator(ho_batch)\n",
        "          d_pred_real = self.discriminator(hc_batch)        \n",
        "          loss += tf.nn.sigmoid_cross_entropy_with_logits(d_batch, d_pred_real)\n",
        "        gradients = tape.gradient(loss, self.discriminator.trainable_variables)\n",
        "        disc_optimizer.apply_gradients(zip(gradients, self.discriminator.trainable_variables))\n",
        "        disc_loss(loss)\n",
        "      disc_loss_log.append(disc_loss.result())\n",
        "\n",
        "      template = 'Epoch {}: discriminator_loss: {}'\n",
        "      print(template.format(epoch + 1, disc_loss.result()), end = \" - \")\n",
        "\n",
        "      occ_loss.reset_states()\n",
        "      occ_accuracy.reset_states()\n",
        "      batch_iter = iter(train_ds)\n",
        "      for k in range(k2):\n",
        "        x_batch, y_batch = next(batch_iter)\n",
        "        occ_x_batch = self.occlusion_creator_2.impose(x_batch.numpy())\n",
        "        ho = self.occluded_net.feature_map(occ_x_batch)\n",
        "        with tf.GradientTape() as tape:\n",
        "          yo = self.occluded_net(occ_x_batch)\n",
        "          yc = self.non_occluded_net(x_batch)\n",
        "          d_ho = self.discriminator(ho)\n",
        "          x_rec = self.decoder(ho)\n",
        "\n",
        "          l_sup = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_batch, yo))  #.2\n",
        "          l_sim = tf.reduce_mean(tf.keras.losses.MSE(yo, yc)) #.125\n",
        "          l_lir = tf.math.maximum(0, tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_batch, yc)) - l_sup)#.175\n",
        "          l_adv = tf.reduce_mean(-tf.math.log(d_ho))#2.5\n",
        "          l_rec = tf.reduce_mean(tf.keras.losses.MSE(tf.reshape(x_batch, (batch_size, -1)), tf.reshape(x_rec, (batch_size, -1)))) #2.5\n",
        "          \n",
        "          loss = l_sup + lambdas[0]*l_sim + lambdas[1]*l_lir + lambdas[2]*l_adv + lambdas[3]*l_rec\n",
        "          # print(l_sup, lambdas[0]*l_sim, lambdas[1]*l_lir, lambdas[2]*l_adv, lambdas[3]*l_rec)\n",
        "        gradients = tape.gradient(loss, self.occ_model.trainable_variables)\n",
        "        occ_optimizer.apply_gradients(zip(gradients, self.occ_model.trainable_variables))\n",
        "\n",
        "        occ_loss(loss)\n",
        "        occ_accuracy(y_batch, yo)\n",
        "      occ_loss_log.append(occ_loss.result())\n",
        "      accuracy_log.append(occ_accuracy.result())\n",
        "\n",
        "      template = 'model_loss: {} - train_accuracy: {} - test_accuracy: {}'\n",
        "      print(template.format(occ_loss.result(), 100*occ_accuracy.result(), self.occluded_net.evaluate(test_ds)[1]))\n",
        "\n",
        "  def evaluate(self, dataset):\n",
        "    evaluation = {}\n",
        "    scores = self.occluded_net.evaluate(dataset)\n",
        "    metrics_names = self.occluded_net.metrics_names\n",
        "    for i in range(len(metrics_names)):\n",
        "      evaluation[metrics_names[i]] = scores[i]\n",
        "    return evaluation\n",
        "\n",
        "  def predict(self, dataset):\n",
        "    pred = self.occluded_net.predict(dataset)\n",
        "    label_dict = {0: 'surprise', 1: 'fear', 2: 'disgust', 3: 'happiness', 4: 'sadness', 5: 'anger', 6: 'neutral'}\n",
        "    predictions = []\n",
        "    for i in range(pred.shape[0]):\n",
        "      predictions.append(label_dict[np.argmax(pred[i])])\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEU27GvdJd4D"
      },
      "source": [
        "model=Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glGPWxWsaLW9"
      },
      "source": [
        "path = '/content/drive/My Drive/Occluded Facial Expression Recognition/Datasets/NEW_OCCLUDED RAF-DB/Images'\n",
        "dataset = Dataset(path)\n",
        "train_dso = dataset.get_train_ds()\n",
        "test_dso = dataset.get_test_ds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuP7mjjwzk1T"
      },
      "source": [
        "model.train_occluded_net(train_dso, test_dso)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NmLH2cYKmB-"
      },
      "source": [
        "path = '/content/drive/My Drive/Occluded Facial Expression Recognition/Datasets/RAF-DB/Images'\n",
        "dataset = Dataset(path)\n",
        "train_ds = dataset.get_train_ds()\n",
        "test_ds = dataset.get_test_ds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOswkWGJKo3k"
      },
      "source": [
        "model.train_non_occluded_net(train_ds, test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27YIP3SrZWcF"
      },
      "source": [
        "lambdas = tf.constant([0.2, 0.175, 0.25, 0.25]) #[0.2, 0.175, 0.75, 0.75]\n",
        "model.train(train_ds, test_ds, lambdas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWtePnbBwCYJ"
      },
      "source": [
        "model.evaluate(test_dso)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}